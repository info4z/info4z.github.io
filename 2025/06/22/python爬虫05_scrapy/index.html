

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="倗诚">
  <meta name="keywords" content="">
  
    <meta name="description" content="爬虫代码不难, 难的是与反爬虫之间的博弈">
<meta property="og:type" content="article">
<meta property="og:title" content="python爬虫之scrapy">
<meta property="og:url" content="https://info4z.github.io/2025/06/22/python%E7%88%AC%E8%99%AB05_scrapy/index.html">
<meta property="og:site_name" content="云深不知处">
<meta property="og:description" content="爬虫代码不难, 难的是与反爬虫之间的博弈">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://info4z.github.io/python/scrapy_architecture.jpg">
<meta property="article:published_time" content="2025-06-21T16:00:00.000Z">
<meta property="article:modified_time" content="2025-06-23T17:20:38.243Z">
<meta property="article:author" content="倗诚">
<meta property="article:tag" content="python">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://info4z.github.io/python/scrapy_architecture.jpg">
  
  
  
  <title>python爬虫之scrapy - 云深不知处</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"info4z.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>云深不知处</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="python爬虫之scrapy"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-06-22 00:00" pubdate>
          2025年6月22日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          17k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          142 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">python爬虫之scrapy</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p><strong>Scrapy是什么？</strong> scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
<p><strong>Scrapy的架构组成</strong></p>
<table>
<thead>
<tr>
<th>角色</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>engine</td>
<td>引擎，自动运行，<strong>无需关注</strong>，会自动组织所有的请求对象，分发给下载器</td>
</tr>
<tr>
<td>downloader</td>
<td>下载器，从引擎处获取到请求对象后，请求数据</td>
</tr>
<tr>
<td>spider</td>
<td>爬虫，Spider类定义了如何爬取某个或某些网站，包括爬取的动作，如：是否跟进链接。以及如何从网页的内容中提取结构化数据，也就是爬取Item。换句话说，Spider就是你定义爬取的动作及分析某个或某些网页的地方</td>
</tr>
<tr>
<td>scheduler</td>
<td>调度器，有自己的调度规则，<strong>无需关注</strong></td>
</tr>
<tr>
<td>item pipeline</td>
<td>管道，最终处理数据的管道，会预留接口供我们处理数据</td>
</tr>
</tbody></table>
<p>item pipeline的典型应用</p>
<ol>
<li>清理HTML数据</li>
<li>验证爬取的数据，如：检查item包含某些字段</li>
<li>查重并丢弃</li>
<li>将爬取结果保存到数据库中</li>
</ol>
<p><strong>Scrapy的工作原理</strong></p>
<p><img src="/../python/scrapy_architecture.jpg" srcset="/img/loading.gif" lazyload alt="工作原理"></p>
<p><strong>工作流程</strong>：</p>
<ol>
<li>engine从爬虫处获取需要爬取初始请求</li>
<li>engine在scheduler中调度请求，并要求抓取下一个请求</li>
<li>scheduler将下一个请求返回给engine</li>
<li>engine发送请求到downloader，中间要通过Downloader Middleware</li>
<li>一旦页面下载完成，downloader就会生成一个包含该页面的response，并将其发送到engine，中途要经过Downloader Middleware</li>
<li>engine从downloader接收响应并将其发送到spider进行处理，中间要经过Spider Middleware</li>
<li>spider处理response，并将抓取的items和新的待处理的request返回给engine，中间要经过Spider Middleware</li>
<li>engine将处理后的Items发送到Item Pipeline，然后将处理后的request发送到scheduler，并请求可能的下一个要抓取的请求。</li>
<li>该过程（从第3步开始）会重复进行，直到调度器没有更多请求。</li>
</ol>
<h2 id="二、安装使用"><a href="#二、安装使用" class="headerlink" title="二、安装使用"></a>二、安装使用</h2><h3 id="（一）安装"><a href="#（一）安装" class="headerlink" title="（一）安装"></a>（一）安装</h3><p>安装Scrapy包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">安装</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">pip install scrapy</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">校验</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy</span><br>Scrapy 2.13.2 - no active project<br></code></pre></td></tr></table></figure>

<h3 id="（二）使用"><a href="#（二）使用" class="headerlink" title="（二）使用"></a>（二）使用</h3><p>创建项目</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">scrapy startproject &lt;project_name&gt;</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy startproject scrapy_demo</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">tree /F</span><br>scrapy_demo<br>    │  scrapy.cfg				# 项目的配置文件<br>    │<br>    └─scrapy_demo				# 项目源代码文件夹<br>        │  items.py				# 定义数据结构的地方,是一个继承自scrapy.Item的类<br>        │  middlewares.py		# 定义中间件, 代理<br>        │  pipelines.py			# 定义数据处理管道,里面只有一个类,用于处理下载数据的后续处理默认300优先级,值越小优先级越高,取值1~1000<br>        │  settings.py			# 配置文件,比如: 是否遵守rebots协议,User-Agent定义等<br>        │  __init__.py<br>        │<br>        └─spiders				# 存放爬虫代码的文件夹<br>                __init__.py<br>                xxx.py			# 由我们自己创建,是实现爬虫核心功能的文件<br></code></pre></td></tr></table></figure>

<p>创建爬虫文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cd</span> scrapy_demo/scrapy_demo/spiders</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">scrapy genspider &lt;spider_name&gt; &lt;domain_name&gt;</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy genspider baidu www.baidu.com</span><br>Created spider &#x27;baidu&#x27; using template &#x27;basic&#x27; in module:<br>  scrapy_demo.spiders.baidu<br></code></pre></td></tr></table></figure>

<p>生成了baidu.py的文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BaiduSpider</span>(scrapy.Spider):			<span class="hljs-comment"># 继承scrapy.Spider类</span><br>    name = <span class="hljs-string">&quot;baidu&quot;</span>							<span class="hljs-comment"># 运行爬虫文件时使用的名字</span><br>    allowed_domains = [<span class="hljs-string">&quot;www.baidu.com&quot;</span>]		<span class="hljs-comment"># 爬虫允许的域名,在爬取的时候,如果不是此域名之下的url会被过滤掉</span><br>    start_urls = [<span class="hljs-string">&quot;https://www.baidu.com&quot;</span>]	<span class="hljs-comment"># 声明了爬虫的起始地址,支持写多个url</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):				<span class="hljs-comment"># 解析数据的回调函数</span><br>        <span class="hljs-built_in">print</span>(response.text)				<span class="hljs-comment"># 响应的是字符串</span><br>        <span class="hljs-built_in">print</span>(response.body)				<span class="hljs-comment"># 响应的是二进制文件</span><br>        <span class="hljs-built_in">input</span> = response.xpath(<span class="hljs-string">&#x27;//input[@id=&quot;su&quot;]&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-built_in">input</span>)						<span class="hljs-comment"># 响应的是selector列表</span><br>		<span class="hljs-built_in">print</span>(<span class="hljs-built_in">input</span>.extract())				<span class="hljs-comment"># 提取的是selector对象的data</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-built_in">input</span>.extract_first())		<span class="hljs-comment"># 提取的是selector列表中的第一个元素的data, 类型str</span><br></code></pre></td></tr></table></figure>

<p>运行爬虫代码</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">scrapy crawl &lt;spider_name&gt;</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy crawl baidu</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">没有任何自定义信息输出,观察报文发现</span><br>[scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://www.baidu.com/robots.txt&gt; (referer: None)<br>[scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: &lt;GET https://www.baidu.com&gt;<br><span class="hljs-meta prompt_"># </span><span class="language-bash">关闭robots协议即可</span><br></code></pre></td></tr></table></figure>

<p>修改settings.py，绕过反爬虫机制</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 不遵守robots.txt规则,注释掉亦可</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure>

<h3 id="（三）调试"><a href="#（三）调试" class="headerlink" title="（三）调试"></a>（三）调试</h3><p><strong>scrapy shell 是什么？</strong> scrapy的交互终端，供您在未启动spider的情况下尝试及调试您的爬取代码。 其本意是用来测试提取数据的代码，不过您可以将其作为正常的Python终端，在上面测试任何的Python代码。该终端是用来测试XPath或CSS表达式，查看他们的工作方式及从爬取的网页中提取的数据。 在编写您的spider时，该终端提供了交互性测试您的表达式代码的功能，免去了每次修改后运行spider的麻烦。</p>
<p>推荐安装ipython</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">建议使用ipython替代标准Python终端,ipython相比更为强大,提供智能的自动补全,高亮输出等</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">pip install ipython</span><br></code></pre></td></tr></table></figure>

<p>基本应用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy shell www.baidu.com</span><br>[s] Available Scrapy objects:<br>[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)<br>[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x000001B2C3F33CB0&gt;<br>[s]   item       &#123;&#125;<br>[s]   request    &lt;GET http://www.baidu.com&gt;<br>[s]   response   &lt;200 http://www.baidu.com&gt;<br>[s]   settings   &lt;scrapy.settings.Settings object at 0x000001B2C3FBC7D0&gt;<br>[s] Useful shortcuts:<br>[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)<br>[s]   fetch(req)                  Fetch a scrapy.Request and update local objects<br>[s]   shelp()           Shell help (print this help)<br>[s]   view(response)    View response in a browserTip: Put a &#x27;;&#x27; at the end of a line to suppress the printing of output.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">以上的对象都可以用</span><br>In [1]: response.url<br>Out[1]: &#x27;http://www.baidu.com&#x27;<br><br>In [1]: response.xpath(&#x27;//input[@id=&quot;su&quot;]&#x27;)<br>Out[1]: [&lt;Selector query=&#x27;//input[@id=&quot;su&quot;]&#x27; data=&#x27;&lt;input type=&quot;submit&quot; id=&quot;su&quot; value=&quot;百...&#x27;&gt;]<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">!语法练习</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">1.response对象：</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">response.body</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">response.text</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">response.url</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">response.status</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">2.response的解析：</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">response.xpath()	常用,使用xpath路径查询特定元素,返回一个selector列表对象</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">response.css()	使用css_selector查询元素,返回一个selector列表对象</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	获取内容 ：response.css(<span class="hljs-string">&#x27;#su::text&#x27;</span>).extract_first()</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	获取属性 ：response.css(<span class="hljs-string">&#x27;#su::attr(“value”)&#x27;</span>).extract_first()</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">3.selector对象（通过xpath方法调用返回的是seletor列表）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	extract()	提取selector对象的值,如果提取不到则会报错,使用xpath请求到的对象是一个selector对象,需要进一步使用extract()方法拆包,转换为unicode字符串</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	extract_first()	提取seletor列表中的第一个值,如果提取不到值,不会报错,而是会返回一个空值</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	xpath()</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	css()</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">! 注意: 每一个selector对象可以再次的去使用xpath或者css方法</span><br></code></pre></td></tr></table></figure>



<h2 id="三、请求"><a href="#三、请求" class="headerlink" title="三、请求"></a>三、请求</h2><p>get请求</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DoubanSpiderSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&quot;douban_spider&quot;</span>  <span class="hljs-comment"># 定义爬虫的名称,必须是唯一的</span><br>    allowed_domains = [<span class="hljs-string">&quot;movie.douban.com&quot;</span>]  <span class="hljs-comment"># 限制爬虫的访问域名,防止爬虫爬取其他域名的网页</span><br>    start_urls = [<span class="hljs-string">&quot;https://movie.douban.com/top250&quot;</span>]  <span class="hljs-comment"># 定义爬虫的起始页面,爬虫将从这些页面开始抓取</span><br><br>    <span class="hljs-comment"># 自定义请求,指定请求头</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">start_requests</span>(<span class="hljs-params">self</span>):<br>        headers = &#123;<br>            <span class="hljs-string">&#x27;Referer&#x27;</span>: <span class="hljs-string">&#x27;https://movie.douban.com/&#x27;</span>,<br>        &#125;<br>        <span class="hljs-comment"># 重写start_request后,可以直接在方法中定义起始url,外面的start_urls其实已经失效了</span><br>        <span class="hljs-comment"># 这里引用一下,多少起点作用</span><br>		url = start_urls[<span class="hljs-number">0</span>]<br>        <span class="hljs-comment"># 发送GET请求,参数: url, headers, callback(数据解析)</span><br>        <span class="hljs-keyword">yield</span> scrapy.Request(url, headers=headers, callback=self.parse)	<span class="hljs-comment"># headers不是必须的</span><br>    <br>    <span class="hljs-comment"># 解析数据</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>

<p>post请求</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1.重写start_requests方法：</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">start_requests</span>(<span class="hljs-params">self</span>)<br><span class="hljs-comment"># 2.start_requests的返回值：</span><br>scrapy.FormRequest(url=url, headers=headers, callback=self.parse_item, formdata=data)<br><span class="hljs-comment"># url: 要发送的post地址</span><br><span class="hljs-comment"># headers：可以定制头信息</span><br><span class="hljs-comment"># callback: 回调函数</span><br><span class="hljs-comment"># formdata: post所携带的数据，这是一个字</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FanyiSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&#x27;fanyi&#x27;</span><br>    allowed_domains = [<span class="hljs-string">&#x27;https://fanyi.baidu.com/sug&#x27;</span>]<br>    <span class="hljs-comment"># post请求 如果没有参数 那么这个请求将没有任何意义</span><br>    <span class="hljs-comment"># 所以start_urls 也没有用了</span><br>    <span class="hljs-comment"># parse方法也没有用了</span><br>    <span class="hljs-comment"># start_urls = [&#x27;https://fanyi.baidu.com/sug/&#x27;]</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># def parse(self, response):</span><br>    <span class="hljs-comment">#     pass</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">start_requests</span>(<span class="hljs-params">self</span>):<br>        url = <span class="hljs-string">&#x27;https://fanyi.baidu.com/sug&#x27;</span><br><br>        data = &#123;<br>            <span class="hljs-string">&#x27;kw&#x27;</span>: <span class="hljs-string">&#x27;final&#x27;</span><br>        &#125;<br>		<span class="hljs-comment"># post请求</span><br>        <span class="hljs-comment"># yield scrapy.Request(</span><br>        <span class="hljs-comment">#     url=url,</span><br>        <span class="hljs-comment">#     method=&#x27;POST&#x27;,</span><br>        <span class="hljs-comment">#     body=json.dumps(data),  # 如果需要 JSON 格式</span><br>        <span class="hljs-comment">#     headers=&#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;&#125;,</span><br>        <span class="hljs-comment">#     callback=self.parse_response</span><br>        <span class="hljs-comment"># )</span><br>        <br>        <span class="hljs-comment"># 推荐使用PormRequest</span><br>        <span class="hljs-keyword">yield</span> scrapy.FormRequest(url=url, formdata=data, callback=self.parse_second)<br>        <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_second</span>(<span class="hljs-params">self,response</span>):<br><br>        content = response.text<br>        obj = json.loads(content, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br><br>        <span class="hljs-built_in">print</span>(obj)<br></code></pre></td></tr></table></figure>



<h2 id="四、CrawlSpider"><a href="#四、CrawlSpider" class="headerlink" title="四、CrawlSpider"></a>四、CrawlSpider</h2><h3 id="（一）概述"><a href="#（一）概述" class="headerlink" title="（一）概述"></a>（一）概述</h3><p><code>CrawlSpider</code> 是一种特殊的 Spider 类，继承自scrapy.Spider，用于处理复杂的爬取规则和链接跟踪。</p>
<p>CrawlSpider可以定义规则，在解析html的时候，可以根据链接规则提取出指定的链接，然后在向这些链接发送请求。因此，如果有跟进链接的需求，比如在爬取了网页之后，需要提取链接再次爬取，使用CrawlSpider非常合适。</p>
<p>因为是特殊的类，所以在创建的时候稍微有所不同</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">创建项目,这个不变: scrapy startproject &lt;project_name&gt;</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy startproject crawlspider_demo</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">创建爬虫: scrapy genspider -t crawl &lt;spider_name&gt; &lt;domain&gt;</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy genspider -t crawl <span class="hljs-built_in">test</span> example.com</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">执行爬虫: scrapy crawl &lt;spider_name&gt;</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy crawl <span class="hljs-built_in">test</span></span><br></code></pre></td></tr></table></figure>

<h3 id="（二）核心方法"><a href="#（二）核心方法" class="headerlink" title="（二）核心方法"></a>（二）核心方法</h3><p>链接提取器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">scrapy.linkextractors.LinkExtractor (<br>    allow = (),				<span class="hljs-comment"># 正则表达式,提取符合正则的链接</span><br>    deny = (),				<span class="hljs-comment"># 正则表达式,不提取符合正则的链接,不用~</span><br>    allow_domains = (),		<span class="hljs-comment"># 允许的域名,不用~</span><br>    deny_domains = (),		<span class="hljs-comment"># 不允许的域名,不用~</span><br>    restrict_xpaths = (),	<span class="hljs-comment"># 提取符合xpath的链接</span><br>    restrict_css = ()		<span class="hljs-comment"># 提取符合选择器规则的链接</span><br>)<br></code></pre></td></tr></table></figure>

<p>使用示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 正则表达式</span><br>LinkExtractor(allow=<span class="hljs-string">r&#x27;list_23_\d+\.html&#x27;</span>)<br><span class="hljs-comment"># xpath表达式</span><br>LinkExtractor(restrict_xpath=<span class="hljs-string">r&#x27;//div[@class=&quot;x&quot;]&#x27;</span>)<br><span class="hljs-comment"># css选择器</span><br>LinkExtractor(restrict_css=<span class="hljs-string">&#x27;.x&#x27;</span>)<br><br><span class="hljs-comment"># 提取链接</span><br>link.extract_links(response)<br></code></pre></td></tr></table></figure>

<h3 id="（三）正则表达式"><a href="#（三）正则表达式" class="headerlink" title="（三）正则表达式"></a>（三）正则表达式</h3><p>字符类</p>
<table>
<thead>
<tr>
<th>元字符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>[abc]</code></td>
<td>匹配 <code>a</code>、<code>b</code> 或 <code>c</code> 中的任意一个字符</td>
</tr>
<tr>
<td><code>[^abc]</code></td>
<td>匹配除 <code>a</code>、<code>b</code>、<code>c</code> 之外的任意一个字符</td>
</tr>
<tr>
<td><code>[a-z]</code></td>
<td>匹配任意一个小写字母</td>
</tr>
<tr>
<td><code>[A-Z]</code></td>
<td>匹配任意一个大写字母</td>
</tr>
<tr>
<td><code>[0-9]</code></td>
<td>匹配任意一个数字</td>
</tr>
<tr>
<td><code>\d</code></td>
<td>匹配任意一个数字，等价于 <code>[0-9]</code></td>
</tr>
<tr>
<td><code>\D</code></td>
<td>匹配任意一个非数字字符，等价于 <code>[^0-9]</code></td>
</tr>
<tr>
<td><code>\w</code></td>
<td>匹配非字母、数字或下划线的字符，等价于 <code>[a-zA-Z0-9_]</code></td>
</tr>
<tr>
<td><code>\s</code></td>
<td>匹配任意空白字符（空格、制表符、换行符等）</td>
</tr>
<tr>
<td><code>\S</code></td>
<td>匹配任意非空白字符</td>
</tr>
</tbody></table>
<p>量词</p>
<table>
<thead>
<tr>
<th>元字符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>?</code></td>
<td>匹配前面的子表达式0次或1次</td>
</tr>
<tr>
<td><code>*</code></td>
<td>匹配前面的子表达式0次或多次</td>
</tr>
<tr>
<td><code>+</code></td>
<td>匹配前面的子表达式1次或多次</td>
</tr>
<tr>
<td><code>&#123;n&#125;</code></td>
<td>匹配前面的子表达式恰好 <code>n</code> 次</td>
</tr>
<tr>
<td><code>&#123;n,&#125;</code></td>
<td>匹配前面的子表达式至少 <code>n</code> 次</td>
</tr>
<tr>
<td><code>&#123;n,m&#125;</code></td>
<td>匹配前面的子表达式至少 <code>n</code> 次，至多 <code>m</code> 次</td>
</tr>
</tbody></table>
<p>边界匹配</p>
<table>
<thead>
<tr>
<th>元字符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>^a</code></td>
<td>匹配以 <code>a</code> 开头的字符串</td>
</tr>
<tr>
<td><code>a$</code></td>
<td>匹配以 <code>a</code> 结尾的字符串</td>
</tr>
</tbody></table>
<h2 id="五、数据存储"><a href="#五、数据存储" class="headerlink" title="五、数据存储"></a>五、数据存储</h2><h3 id="（一）pymysql"><a href="#（一）pymysql" class="headerlink" title="（一）pymysql"></a>（一）pymysql</h3><p>安装pymysql库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">pip install pymysql</span><br></code></pre></td></tr></table></figure>

<p>常用方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 连接数据库</span><br>conn = pymysql.connect(host, port, user, password, db, charset)<br><span class="hljs-comment"># 获取指针</span><br>cursor = conn.cursor()<br><span class="hljs-comment"># 执行sql</span><br>cursor.execute()<br></code></pre></td></tr></table></figure>

<h3 id="（二）数据入库"><a href="#（二）数据入库" class="headerlink" title="（二）数据入库"></a>（二）数据入库</h3><p>settings配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">DB_HOST = <span class="hljs-string">&#x27;192.168.231.128&#x27;</span><br>DB_PORT = <span class="hljs-number">3306</span><br>DB_USER = <span class="hljs-string">&#x27;root&#x27;</span><br>DB_PASSWORD = <span class="hljs-string">&#x27;1234&#x27;</span><br>DB_NAME = <span class="hljs-string">&#x27;test&#x27;</span><br>DB_CHARSET = <span class="hljs-string">&#x27;utf8&#x27;</span><br></code></pre></td></tr></table></figure>

<p>pipelines编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scrapy.utils.project <span class="hljs-keyword">import</span> get_project_settings<br><span class="hljs-keyword">import</span> pymysql<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MysqlPipeline</span>(<span class="hljs-title class_ inherited__">object</span>):<br><br>    <span class="hljs-comment"># 构造函数,用open_spider效果一样</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        settings = get_project_settings()<br>        self.host = settings[<span class="hljs-string">&#x27;DB_HOST&#x27;</span>]<br>        self.port = settings[<span class="hljs-string">&#x27;DB_PORT&#x27;</span>]<br>        self.user = settings[<span class="hljs-string">&#x27;DB_USER&#x27;</span>]<br>        self.pwd = settings[<span class="hljs-string">&#x27;DB_PWD&#x27;</span>]<br>        self.name = settings[<span class="hljs-string">&#x27;DB_NAME&#x27;</span>]<br>        self.charset = settings[<span class="hljs-string">&#x27;DB_CHARSET&#x27;</span>]<br>        self.connect()<br><br>    <span class="hljs-comment"># 连接数据库并且获取cursor对象</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">connect</span>(<span class="hljs-params">self</span>):<br>        self.conn = pymysql.connect(host=self.host,<br>                                    port=self.port,<br>                                    user=self.user,<br>                                    password=self.pwd,<br>                                    db=self.name,<br>                                    charset=self.charset)<br>        self.cursor = self.conn.cursor()<br><br>    <span class="hljs-comment"># 处理数据</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        <span class="hljs-comment"># sql = &#x27;insert into book(image_url, book_name, author, info) values(&quot;%s&quot;,</span><br><span class="hljs-string">&quot;%s&quot;</span>, <span class="hljs-string">&quot;%s&quot;</span>, <span class="hljs-string">&quot;%s&quot;</span>)<span class="hljs-string">&#x27; % (item[&#x27;</span>image_url<span class="hljs-string">&#x27;], item[&#x27;</span>book_name<span class="hljs-string">&#x27;], item[&#x27;</span>autho<span class="hljs-string">r&#x27;], item[&#x27;</span>info<span class="hljs-string">&#x27;])</span><br><span class="hljs-string">sql = &#x27;</span>insert into book(image_url,book_name,author,info) values<br>(<span class="hljs-string">&quot;&#123;&#125;&quot;</span>,<span class="hljs-string">&quot;&#123;&#125;&quot;</span>,<span class="hljs-string">&quot;&#123;&#125;&quot;</span>,<span class="hljs-string">&quot;&#123;&#125;&quot;</span>)<span class="hljs-string">&#x27;.format(item[&#x27;</span>image_url<span class="hljs-string">&#x27;], item[&#x27;</span>book_name<span class="hljs-string">&#x27;], item[&#x27;</span>autho<span class="hljs-string">r&#x27;],</span><br><span class="hljs-string">item[&#x27;</span>info<span class="hljs-string">&#x27;])</span><br><span class="hljs-string">		# 执行sql语句</span><br><span class="hljs-string">    	self.cursor.execute(sql)</span><br><span class="hljs-string">        sefl.cursor.commit()</span><br><span class="hljs-string">        return item</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    def close_spider(self, spider):</span><br><span class="hljs-string">        self.cursor.close()</span><br><span class="hljs-string">        self.conn.close()</span><br></code></pre></td></tr></table></figure>



<h2 id="六、代理"><a href="#六、代理" class="headerlink" title="六、代理"></a>六、代理</h2><p>在 Scrapy 中使用代理可以帮助你隐藏真实 IP 地址，避免被目标网站封禁，或者用于访问特定地区的网络内容。</p>
<p>修改settings配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">DOWNLOADER_MIDDLEWARES = &#123;<br>    <span class="hljs-string">&quot;XXX.middlewares.XXXDownloaderMiddleware&quot;</span>: <span class="hljs-number">543</span>,<br>&#125;<br></code></pre></td></tr></table></figure>

<p>修改middlewares.py中的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">XXXSpiderMiddleware</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_request</span>(<span class="hljs-params">self, request, spider</span>):<br>        request.meta[<span class="hljs-string">&#x27;proxy&#x27;</span>] = <span class="hljs-string">&#x27;https://113.68.202.10:9999&#x27;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>



<h2 id="七、日志"><a href="#七、日志" class="headerlink" title="七、日志"></a>七、日志</h2><p><strong>日志级别</strong>：默认是DEBUG，只要出现了DEBUG或以上等级的都会被打印。具体级别从高到低分别为：</p>
<ul>
<li>CRITICAL：危险的，验证错误</li>
<li>ERROR：一般错误</li>
<li>WARNING：警告</li>
<li>INFO：一般信息</li>
<li>DEBUG：调试信息</li>
</ul>
<p><strong>日志文件</strong>：通常我们希望日志可以保存到一个文件中，方便查阅和追溯，scrapy允许我们在settings中编写配置信息达成目的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将控制台信息记录到文件中,屏幕不再显示,文件后缀一定是.log</span><br>LOG_FILE: scrapy_test.log<br><span class="hljs-comment"># 设置日志级别,推荐用默认值</span><br>LOG_LEVEL: DEBUG<br></code></pre></td></tr></table></figure>



<h2 id="八、实战演练"><a href="#八、实战演练" class="headerlink" title="八、实战演练"></a>八、实战演练</h2><h3 id="（一）提取《58同城》工作岗位的数据"><a href="#（一）提取《58同城》工作岗位的数据" class="headerlink" title="（一）提取《58同城》工作岗位的数据"></a>（一）提取《58同城》工作岗位的数据</h3><p>需求：搜索”前端开发“，提取所有工作的标题</p>
<blockquote>
<p>域名：bj.58.com</p>
<p>目的：练习response的属性和方法</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TcSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&quot;tc&quot;</span><br>    allowed_domains = [<span class="hljs-string">&quot;bj.58.com&quot;</span>]<br>    start_urls = [<span class="hljs-string">&quot;https://bj.58.com/sou/?key=%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91&quot;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-comment"># ret = response.text</span><br>        <span class="hljs-comment"># ret = response.body</span><br>        <span class="hljs-comment"># print(ret)</span><br>        ret = response.xpath(<span class="hljs-string">&#x27;//tr[starts-with(@class, &quot;listitem&quot;)]//a[@class=&quot;t&quot;]/@title&#x27;</span>)<br>        titles = ret.extract()<br>        title = ret.extract_first()<br>        <span class="hljs-built_in">print</span>(titles, title)<br></code></pre></td></tr></table></figure>

<h3 id="（二）提取《汽车之家》的车辆信息"><a href="#（二）提取《汽车之家》的车辆信息" class="headerlink" title="（二）提取《汽车之家》的车辆信息"></a>（二）提取《汽车之家》的车辆信息</h3><p>需求：搜索”沃尔沃“，提取当前页所有车辆的名称和价格</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 注意: 如果url是静态文件,不能以/结尾</span><br><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CarSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&quot;car&quot;</span><br>    allowed_domains = [<span class="hljs-string">&quot;car.autohome.com.cn&quot;</span>]<br>    start_urls = [<span class="hljs-string">&quot;https://car.autohome.com.cn/price/list-0-0-0-0-0-0-0-0-70-0-0-0-0-0-0-1.html&quot;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        names = response.xpath(<span class="hljs-string">&#x27;//div[@class=&quot;list-cont&quot;]//a[@class=&quot;font-bold&quot;]/text()&#x27;</span>).extract()<br>        prices = response.xpath(<span class="hljs-string">&#x27;//div[@class=&quot;list-cont&quot;]//span[@class=&quot;font-arial&quot;]/text()&#x27;</span>).extract()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(names)):<br>            <span class="hljs-built_in">print</span>(names[i], prices[i])<br></code></pre></td></tr></table></figure>



<h3 id="（三）提取《当当网》书籍的信息"><a href="#（三）提取《当当网》书籍的信息" class="headerlink" title="（三）提取《当当网》书籍的信息"></a>（三）提取《当当网》书籍的信息</h3><p>需求：下载图书&#x2F;青春文学&#x2F;青春爱情文学下书籍的名称、图片、价格</p>
<blockquote>
<p>域名：<a target="_blank" rel="noopener" href="http://category.dangdang.com/">http://category.dangdang.com</a></p>
<p>目标：理解scrapy的工作原理</p>
</blockquote>
<p>创建项目，创建爬虫</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy startproject dang</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cd</span> dang</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy genspider dang_spider category.dangdang.com</span><br></code></pre></td></tr></table></figure>

<p>编写dang_spider.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> dang.items <span class="hljs-keyword">import</span> DangItem<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DangSpiderSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&quot;dang_spider&quot;</span><br>    allowed_domains = [<span class="hljs-string">&quot;category.dangdang.com&quot;</span>]<br>    start_urls = [<span class="hljs-string">&quot;http://category.dangdang.com/cp01.01.02.00.00.00.html&quot;</span>]<br><br>    base_url = <span class="hljs-string">&#x27;http://category.dangdang.com/pg&#x27;</span><br>    page = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-comment"># 1.获取数据</span><br>        lis = response.xpath(<span class="hljs-string">&#x27;//ul[@id=&quot;component_59&quot;]/li&#x27;</span>)<br>        <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> lis:<br>            name = li.xpath(<span class="hljs-string">&#x27;./a/img/@alt&#x27;</span>).extract_first().strip().replace(<span class="hljs-string">&#x27;/&#x27;</span>, <span class="hljs-string">&#x27;&amp;&#x27;</span>)<br>            image = li.xpath(<span class="hljs-string">&#x27;./a/img/@data-original&#x27;</span>).extract_first()<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> image:  <span class="hljs-comment"># 如果没有data-original则提取src属性</span><br>                image = li.xpath(<span class="hljs-string">&#x27;./a/img/@src&#x27;</span>).extract_first()<br>            price = li.xpath(<span class="hljs-string">&#x27;./p[@class=&quot;price&quot;]/span[1]/text()&#x27;</span>).extract_first()<br>            <span class="hljs-comment"># 看一下数据是否有效</span><br>            <span class="hljs-comment"># print(name, image, price)</span><br><br>            <span class="hljs-comment"># 2，编写item,封装对象</span><br>            book = DangItem(name=name, image=image, price=price)<br>            <span class="hljs-comment"># 3.yield, 因为是item, 会被engine交给pipeline处理</span><br>            <span class="hljs-keyword">yield</span> book<br><br>        <span class="hljs-comment"># 5.多页下载: /pg2-cp01.01.02.00.00.00.html</span><br>        ext = <span class="hljs-string">&#x27;-cp01.01.02.00.00.00.html&#x27;</span><br>        <span class="hljs-keyword">if</span> self.page &lt; <span class="hljs-number">100</span>:<br>            self.page += <span class="hljs-number">1</span><br>            url = self.base_url + <span class="hljs-built_in">str</span>(self.page) + ext<br>            <span class="hljs-comment"># yield, 因为是请求, 所以会被engine交给scheduler处理</span><br>            <span class="hljs-keyword">yield</span> scrapy.Request(url=url, callback=self.parse)<br></code></pre></td></tr></table></figure>

<p>编写items.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DangItem</span>(scrapy.Item):<br>    <span class="hljs-comment"># 2.定义数据结构, 指定item的属性</span><br>    name = scrapy.Field()<br>    image = scrapy.Field()<br>    price = scrapy.Field()<br></code></pre></td></tr></table></figure>

<p>编写pipelines.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><br><span class="hljs-comment"># 要使用管道,需要先在settings中开启</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DangPipeline</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">open_spider</span>(<span class="hljs-params">self, spider</span>):<br>        self.file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;book.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br><br>    <span class="hljs-comment"># 3.处理提取的数据</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        <span class="hljs-comment"># with open(&#x27;book.json&#x27;, &#x27;a&#x27;, encoding=&#x27;utf8&#x27;) as fp:</span><br>        <span class="hljs-comment">#     fp.write(str(item))</span><br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        这里有个问题,每次接到一个book,都需要打开一次文件,IO太频繁了</span><br><span class="hljs-string">        优化方案:</span><br><span class="hljs-string">            1.在spider启动时打开文件 =&gt; open_spider()</span><br><span class="hljs-string">            2.在spider关闭时关闭文件 =&gt; close_spider()</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        self.file.write(<span class="hljs-built_in">str</span>(item) + <span class="hljs-string">&#x27;,&#x27;</span>)<br>        <span class="hljs-keyword">return</span> item<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close_spider</span>(<span class="hljs-params">self, spider</span>):<br>        self.file.close()<br><br><br><span class="hljs-keyword">import</span> urllib.request<br><br><span class="hljs-comment"># 4.多条管道开启: 定义新的管道类,在settings.py中开启</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DangDownloadPipeline</span>:<br>    <span class="hljs-comment"># 下载图片</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        url = <span class="hljs-string">&#x27;http:&#x27;</span> + item.get(<span class="hljs-string">&#x27;image&#x27;</span>)<br>        filename = <span class="hljs-string">&#x27;./book/&#x27;</span> + item.get(<span class="hljs-string">&#x27;name&#x27;</span>).strip() + <span class="hljs-string">&#x27;.jpg&#x27;</span>  <span class="hljs-comment"># 注意目录名前面的&quot;.&quot;代表的是执行`csrapy crawl`的目录</span><br>        urllib.request.urlretrieve(url=url, filename=filename)<br>        <span class="hljs-keyword">return</span> item<br></code></pre></td></tr></table></figure>

<p>修改settings.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 关闭rebots协议</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><span class="hljs-comment"># 配置管道</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&quot;dang.pipelines.DangPipeline&quot;</span>: <span class="hljs-number">300</span>,<br>   <span class="hljs-string">&quot;dang.pipelines.DangDownloadPipeline&quot;</span>: <span class="hljs-number">301</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p>运行项目</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cd</span> dang</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy crawl dang_spider</span><br></code></pre></td></tr></table></figure>

<h3 id="（四）提取《电影天堂》电影信息"><a href="#（四）提取《电影天堂》电影信息" class="headerlink" title="（四）提取《电影天堂》电影信息"></a>（四）提取《电影天堂》电影信息</h3><p>需求：提取”国内电影“第一页的所有电影名称和其详情中的图片信息</p>
<blockquote>
<p>域名：dydytt.net</p>
<p>目的：练习二次提取如何传参</p>
</blockquote>
<p>spider编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">from</span> dytt.items <span class="hljs-keyword">import</span> DyttItem<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MovieSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&quot;movie&quot;</span><br>    allowed_domains = [<span class="hljs-string">&quot;dydytt.net&quot;</span>]<br>    start_urls = [<span class="hljs-string">&quot;https://dydytt.net/html/gndy/china/index.html&quot;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-comment"># 1.获取电影名称和详情的url</span><br>        a_list = response.xpath(<span class="hljs-string">&#x27;//table[@class=&quot;tbspan&quot;]//a[@class=&quot;ulink&quot;][2]&#x27;</span>)<br>        <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> a_list:<br>            name = a.xpath(<span class="hljs-string">&#x27;./text()&#x27;</span>).extract_first()<br>            url = <span class="hljs-string">&#x27;https://dydytt.net&#x27;</span> + a.xpath(<span class="hljs-string">&#x27;./@href&#x27;</span>).extract_first()<br><br>            <span class="hljs-keyword">yield</span> scrapy.Request(url=url, callback=self.second_parse, meta=&#123;<span class="hljs-string">&#x27;name&#x27;</span>: name&#125;)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">second_parse</span>(<span class="hljs-params">self, response</span>):<br>        src = response.xpath(<span class="hljs-string">&#x27;//div[@id=&quot;Zoom&quot;]//img/@src&#x27;</span>).extract_first()<br>        <span class="hljs-comment"># print(src, type(src))</span><br>        name = response.meta[<span class="hljs-string">&#x27;name&#x27;</span>]<br>        <span class="hljs-comment"># 2.封装对象, pipeline处理</span><br>        <span class="hljs-keyword">yield</span> DyttItem(name=name, src=src)<br></code></pre></td></tr></table></figure>

<p>item编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DyttItem</span>(scrapy.Item):<br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    <span class="hljs-comment"># name = scrapy.Field()</span><br>    name = scrapy.Field()<br>    src = scrapy.Field()<br></code></pre></td></tr></table></figure>

<p>pipeline编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DyttPipeline</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">open_spider</span>(<span class="hljs-params">self, spider</span>):<br>        self.file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;movie.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf8&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        self.file.write(<span class="hljs-built_in">str</span>(item))<br>        <span class="hljs-keyword">return</span> item<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close_spider</span>(<span class="hljs-params">self, spider</span>):<br>        self.file.close()<br></code></pre></td></tr></table></figure>



<h3 id="（五）提取《读书网》的小说信息"><a href="#（五）提取《读书网》的小说信息" class="headerlink" title="（五）提取《读书网》的小说信息"></a>（五）提取《读书网》的小说信息</h3><p>需求：提取当代小说类目下所有书籍的名称和图片，写入到我们的数据库中</p>
<blockquote>
<p>域名：<a target="_blank" rel="noopener" href="https://dushu.com/">https://dushu.com</a></p>
<p>目的：熟悉CrawlSpider的语法、练习pymysql的使用</p>
</blockquote>
<p>编写爬虫</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> scrapy.linkextractors <span class="hljs-keyword">import</span> LinkExtractor<br><span class="hljs-keyword">from</span> scrapy.spiders <span class="hljs-keyword">import</span> CrawlSpider, Rule<br><span class="hljs-keyword">from</span> dushu.items <span class="hljs-keyword">import</span> DushuItem<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReadSpider</span>(<span class="hljs-title class_ inherited__">CrawlSpider</span>):<br>    name = <span class="hljs-string">&quot;read&quot;</span><br>    allowed_domains = [<span class="hljs-string">&quot;www.dushu.com&quot;</span>]<br>    start_urls = [<span class="hljs-string">&quot;https://www.dushu.com/book/1188.html&quot;</span>]<br><br>    rules = (Rule(LinkExtractor(allow=<span class="hljs-string">r&quot;/book/1188_\d+.html&quot;</span>),<br>                  callback=<span class="hljs-string">&quot;parse_item&quot;</span>,<br>                  follow=<span class="hljs-literal">True</span>),)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_item</span>(<span class="hljs-params">self, response</span>):<br>        img_list = response.xpath(<span class="hljs-string">&#x27;//div[@class=&quot;book-info&quot;]//img&#x27;</span>)<br>        <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> img_list:<br>            name = img.xpath(<span class="hljs-string">&#x27;./@alt&#x27;</span>).extract_first()<br>            src = img.xpath(<span class="hljs-string">&#x27;./@data-original&#x27;</span>).extract_first()<br>            <span class="hljs-keyword">yield</span> DushuItem(name=name, src=src)<br></code></pre></td></tr></table></figure>

<p>编写item</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DushuItem</span>(scrapy.Item):<br>    name = scrapy.Field()<br>    src = scrapy.Field()<br></code></pre></td></tr></table></figure>

<p>编写settings</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 关闭robots协议</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># 数据库连接信息</span><br>DB_HOST = <span class="hljs-string">&#x27;10.0.0.10&#x27;</span><br>DB_PORT = <span class="hljs-number">3306</span><br>DB_USER = <span class="hljs-string">&#x27;zhang&#x27;</span><br>DB_PASSWORD = <span class="hljs-string">&#x27;Zhang@123&#x27;</span><br>DB_NAME = <span class="hljs-string">&#x27;spider&#x27;</span><br>DB_CHARSET = <span class="hljs-string">&#x27;utf8&#x27;</span><br><br><span class="hljs-comment"># 开启pipeline</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&quot;dushu.pipelines.DushuPipeline&quot;</span>: <span class="hljs-number">300</span>,<br>&#125;<br></code></pre></td></tr></table></figure>

<p>编写pipeline</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><span class="hljs-keyword">from</span> scrapy.utils.project <span class="hljs-keyword">import</span> get_project_settings<br><span class="hljs-keyword">import</span> pymysql<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DushuPipeline</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">open_spider</span>(<span class="hljs-params">self, spider</span>):<br>        settings = get_project_settings()<br>        self.host = settings[<span class="hljs-string">&#x27;DB_HOST&#x27;</span>]<br>        self.port = settings[<span class="hljs-string">&#x27;DB_PORT&#x27;</span>]<br>        self.user = settings[<span class="hljs-string">&#x27;DB_USER&#x27;</span>]<br>        self.password = settings[<span class="hljs-string">&#x27;DB_PASSWORD&#x27;</span>]<br>        self.name = settings[<span class="hljs-string">&#x27;DB_NAME&#x27;</span>]<br>        self.charset = settings[<span class="hljs-string">&#x27;DB_CHARSET&#x27;</span>]<br>        self.connect()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">connect</span>(<span class="hljs-params">self</span>):<br>        self.conn = pymysql.connect(<br>            host=self.host,<br>            port=self.port,<br>            user=self.user,<br>            password=self.password,<br>            db=self.name,<br>            charset=self.charset<br>        )<br>        self.cursor = self.conn.cursor()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        sql = <span class="hljs-string">&#x27;insert into book(name,src) values (&quot;&#123;&#125;&quot;,&quot;&#123;&#125;&quot;)&#x27;</span>.<span class="hljs-built_in">format</span>(item[<span class="hljs-string">&#x27;name&#x27;</span>], item[<span class="hljs-string">&#x27;src&#x27;</span>])<br>        self.cursor.execute(sql)<br>        self.conn.commit()<br>        <span class="hljs-keyword">return</span> item<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close_spider</span>(<span class="hljs-params">self, spider</span>):<br>        self.cursor.close()<br>        self.conn.close()<br></code></pre></td></tr></table></figure>


                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E7%88%AC%E8%99%AB/" class="category-chain-item">爬虫</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/python/">#python</a>
      
        <a href="/tags/%E7%88%AC%E8%99%AB/">#爬虫</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>python爬虫之scrapy</div>
      <div>https://info4z.github.io/2025/06/22/python爬虫05_scrapy/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>倗诚</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年6月22日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/06/21/python%E7%88%AC%E8%99%AB04_requests/" title="python爬虫之requests">
                        <span class="hidden-mobile">python爬虫之requests</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  



  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
